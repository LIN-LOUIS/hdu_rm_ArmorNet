# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
åœ¨å›¾åƒã€è§†é¢‘ã€ç›®å½•ã€é€šé…ç¬¦ã€YouTubeã€æ‘„åƒå¤´ã€æµåª’ä½“ç­‰ä¸Šè¿è¡Œæ¨ç†ã€‚

ä½¿ç”¨ç¤ºä¾‹ - è¾“å…¥æº:
    $ yolo mode=predict model=yolo11n.pt source=0                               # æ‘„åƒå¤´
                                                img.jpg                         # å•å¼ å›¾åƒ
                                                vid.mp4                         # è§†é¢‘æ–‡ä»¶
                                                screen                          # å±å¹•æˆªå›¾
                                                path/                           # å›¾åƒæ–‡ä»¶å¤¹
                                                list.txt                        # å›¾åƒè·¯å¾„åˆ—è¡¨
                                                list.streams                    # æµåª’ä½“åˆ—è¡¨
                                                'path/*.jpg'                    # é€šé…ç¬¦è·¯å¾„
                                                'https://youtu.be/LNwODJXcvt4'  # YouTube è§†é¢‘
                                                'rtsp://example.com/media.mp4'  # RTSPã€RTMPã€HTTP æˆ– TCP æµ

ä½¿ç”¨ç¤ºä¾‹ - æ¨¡å‹æ ¼å¼:
    $ yolo mode=predict model=yolo11n.pt                 # PyTorch æ ¼å¼
                              yolo11n.torchscript        # TorchScript æ ¼å¼
                              yolo11n.onnx               # ONNX Runtime æˆ– OpenCV DNN(ä½¿ç”¨ dnn=True)
                              yolo11n_openvino_model     # OpenVINO
                              yolo11n.engine             # TensorRT
                              yolo11n.mlpackage          # CoreML(ä»… macOS)
                              yolo11n_saved_model        # TensorFlow SavedModel
                              yolo11n.pb                 # TensorFlow GraphDef
                              yolo11n.tflite             # TensorFlow Lite
                              yolo11n_edgetpu.tflite     # TensorFlow Edge TPU
                              yolo11n_paddle_model       # PaddlePaddle
                              yolo11n.mnn                # MNN
                              yolo11n_ncnn_model         # NCNN
                              yolo11n_imx_model          # Sony IMX
                              yolo11n_rknn_model         # Rockchip RKNN
"""

from __future__ import annotations

import platform
import re
import threading
from pathlib import Path
from typing import Any

import cv2
import numpy as np
import torch

from ultralytics.cfg import get_cfg, get_save_dir
from ultralytics.data import load_inference_source
from ultralytics.data.augment import LetterBox
from ultralytics.nn.autobackend import AutoBackend
from ultralytics.utils import DEFAULT_CFG, LOGGER, MACOS, WINDOWS, callbacks, colorstr, ops
from ultralytics.utils.checks import check_imgsz, check_imshow
from ultralytics.utils.files import increment_path
from ultralytics.utils.torch_utils import attempt_compile, select_device, smart_inference_mode
from ultralytics.engine.results import Results
from ultralytics.models.digit_classifier import classify_rois
STREAM_WARNING = """
å¦‚æœæœªä¼ å…¥ `stream=True` å‚æ•°ï¼Œæ¨ç†ç»“æœä¼šä¸æ–­ç´¯ç§¯åœ¨å†…å­˜(RAM)ä¸­ï¼Œ
å¯¹äºè¾ƒå¤§çš„è¾“å…¥æºæˆ–é•¿æ—¶é—´è¿è¡Œçš„è§†é¢‘æµï¼Œå¯èƒ½ä¼šå¯¼è‡´å†…å­˜æº¢å‡ºã€‚
è¯¦æƒ…è¯·å‚è€ƒï¼šhttps://docs.ultralytics.com/modes/predict/

ç¤ºä¾‹:
    results = model(source=..., stream=True)  # ç»“æœç”Ÿæˆå™¨
    for r in results:
        boxes = r.boxes  # æ£€æµ‹æ¡†å¯¹è±¡
        masks = r.masks  # åˆ†å‰²æ©ç å¯¹è±¡
        probs = r.probs  # åˆ†ç±»æ¦‚ç‡å¯¹è±¡
"""


class BasePredictor:
    """
    åŸºç¡€é¢„æµ‹å™¨ç±»ã€‚

    è¯¥ç±»ä¸ºå„ç§æ¨ç†ä»»åŠ¡æä¾›åŸºç¡€åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ¨¡å‹åŠ è½½ã€æ¨ç†æ‰§è¡Œå’Œç»“æœå¤„ç†ï¼Œ
    å¯é€‚ç”¨äºå¤šç§è¾“å…¥æº(å›¾åƒã€è§†é¢‘ã€æµåª’ä½“ç­‰)ã€‚

    å±æ€§:
        args (SimpleNamespace): æ¨ç†é…ç½®å‚æ•°ã€‚
        save_dir (Path): ä¿å­˜ç»“æœçš„è·¯å¾„ã€‚
        done_warmup (bool): æ¨¡å‹æ˜¯å¦å®Œæˆé¢„çƒ­ã€‚
        model (torch.nn.Module): ç”¨äºæ¨ç†çš„æ¨¡å‹ã€‚
        data (dict): æ•°æ®é…ç½®ã€‚
        device (torch.device): æ¨ç†ä½¿ç”¨çš„è®¾å¤‡ã€‚
        dataset (Dataset): ç”¨äºæ¨ç†çš„æ•°æ®é›†å¯¹è±¡ã€‚
        vid_writer (dict[str, cv2.VideoWriter]): è§†é¢‘è¾“å‡ºå†™å…¥å™¨ï¼Œé”®ä¸ºä¿å­˜è·¯å¾„ã€‚
        plotted_img (np.ndarray): æœ€è¿‘ä¸€æ¬¡ç»˜åˆ¶çš„å›¾åƒã€‚
        source_type (SimpleNamespace): è¾“å…¥æºç±»å‹ã€‚
        seen (int): å·²å¤„ç†çš„å›¾åƒæ•°é‡ã€‚
        windows (list[str]): ç”¨äºæ˜¾ç¤ºçš„çª—å£åˆ—è¡¨ã€‚
        batch (tuple): å½“å‰æ‰¹æ¬¡æ•°æ®ã€‚
        results (list[Any]): å½“å‰æ‰¹æ¬¡æ¨ç†ç»“æœã€‚
        transforms (callable): å›¾åƒé¢„å¤„ç†è½¬æ¢å‡½æ•°ã€‚
        callbacks (dict[str, list[callable]]): å„äº‹ä»¶çš„å›è°ƒå‡½æ•°ã€‚
        txt_path (Path): æ–‡æœ¬ç»“æœä¿å­˜è·¯å¾„ã€‚
        _lock (threading.Lock): å¤šçº¿ç¨‹å®‰å…¨é”ã€‚

    æ–¹æ³•:
        preprocess: æ¨ç†å‰å¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„å¤„ç†ã€‚
        inference: è¿è¡Œæ¨¡å‹æ¨ç†ã€‚
        postprocess: å¯¹åŸå§‹é¢„æµ‹ç»“æœè¿›è¡Œåå¤„ç†ã€‚
        predict_cli: åœ¨å‘½ä»¤è¡Œæ¨¡å¼ä¸‹è¿è¡Œæ¨ç†ã€‚
        setup_source: è®¾ç½®è¾“å…¥æºåŠæ¨ç†æ¨¡å¼ã€‚
        stream_inference: åœ¨æµåª’ä½“ä¸Šå®æ—¶æ¨ç†ã€‚
        setup_model: åˆå§‹åŒ–å¹¶é…ç½®æ¨¡å‹ã€‚
        write_results: å°†æ¨ç†ç»“æœå†™å…¥æ–‡ä»¶ã€‚
        save_predicted_images: ä¿å­˜å¸¦é¢„æµ‹ç»“æœçš„å¯è§†åŒ–å›¾åƒã€‚
        show: æ˜¾ç¤ºç»“æœã€‚
        run_callbacks: è¿è¡Œäº‹ä»¶å›è°ƒã€‚
        add_callback: æ·»åŠ æ–°çš„å›è°ƒå‡½æ•°ã€‚
    """

    def __init__(
        self,
        cfg=DEFAULT_CFG,
        overrides: dict[str, Any] | None = None,
        _callbacks: dict[str, list[callable]] | None = None,
    ):
        """
        åˆå§‹åŒ– BasePredictor ç±»ã€‚

        å‚æ•°:
            cfg (str | dict): é…ç½®æ–‡ä»¶è·¯å¾„æˆ–é…ç½®å­—å…¸ã€‚
            overrides (dict, å¯é€‰): é…ç½®é¡¹è¦†ç›–ã€‚
            _callbacks (dict, å¯é€‰): å›è°ƒå‡½æ•°å­—å…¸ã€‚
        """
        self.args = get_cfg(cfg, overrides)
        self.save_dir = get_save_dir(self.args)
        if self.args.conf is None:
            self.args.conf = 0.25  # é»˜è®¤ç½®ä¿¡åº¦é˜ˆå€¼ 0.25
        self.done_warmup = False
        if self.args.show:
            self.args.show = check_imshow(warn=True)

        # åˆå§‹åŒ–é»˜è®¤å˜é‡(åœ¨ setup åæ‰èƒ½ä½¿ç”¨)
        self.model = None
        self.data = self.args.data  # æ•°æ®é…ç½®å­—å…¸
        self.imgsz = None
        self.device = None
        self.dataset = None
        self.vid_writer = {}  # è§†é¢‘å†™å…¥å™¨å­—å…¸ {save_path: writer}
        self.plotted_img = None
        self.source_type = None
        self.seen = 0
        self.windows = []
        self.batch = None
        self.results = None
        self.transforms = None
        self.callbacks = _callbacks or callbacks.get_default_callbacks()
        self.txt_path = None
        self._lock = threading.Lock()  # ç¡®ä¿å¤šçº¿ç¨‹æ¨ç†å®‰å…¨
        callbacks.add_integration_callbacks(self)
        #æ·»åŠ  ROI ç¼“å­˜å±æ€§
        self.armor_rois = []
        def _rbox_to_quad(self, cx, cy, w, h, angle_rad):
            """å°† (cx, cy, w, h, theta[rad]) è½¬ä¸ºå››è¾¹å½¢å››ç‚¹ï¼Œé¡ºåºï¼šlt, rt, rb, lb"""
            cos_t, sin_t = np.cos(angle_rad), np.sin(angle_rad)
            dx, dy = w / 2.0, h / 2.0
            corners = np.array([[-dx, -dy], [dx, -dy], [dx, dy], [-dx, dy]], dtype=np.float32)
            R = np.array([[cos_t, -sin_t], [sin_t, cos_t]], dtype=np.float32)
            rot = corners @ R.T
            rot[:, 0] += cx
            rot[:, 1] += cy
            return rot.astype(np.float32)

        def _crop_quad(self, im0, quad, out_size=(64, 64)):
            """å°†ä»»æ„å››è¾¹å½¢ ROI é€è§†åˆ°å›ºå®šå°å›¾ï¼ˆé»˜è®¤ 64x64ï¼‰"""
            dst = np.array([[0, 0], [out_size[0]-1, 0], [out_size[0]-1, out_size[1]-1], [0, out_size[1]-1]], dtype=np.float32)
            M = cv2.getPerspectiveTransform(quad.astype(np.float32), dst)
            roi = cv2.warpPerspective(im0, M, out_size)
            return roi
 
    def preprocess(self, im: torch.Tensor | list[np.ndarray]) -> torch.Tensor:
        """
        åœ¨æ¨ç†å‰å¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„å¤„ç†ã€‚

        å‚æ•°:
            im (torch.Tensor | list[np.ndarray]): è¾“å…¥å›¾åƒï¼Œ
                è‹¥ä¸ºå¼ é‡åˆ™å½¢çŠ¶ä¸º (N, 3, H, W)ï¼Œ
                è‹¥ä¸ºåˆ—è¡¨åˆ™ä¸º [(H, W, 3) Ã— N]ã€‚

        è¿”å›:
            (torch.Tensor): é¢„å¤„ç†åçš„å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º (N, 3, H, W)ã€‚
        """
        not_tensor = not isinstance(im, torch.Tensor)
        if not_tensor:
            im = np.stack(self.pre_transform(im))
            if im.shape[-1] == 3:
                im = im[..., ::-1]  # BGR è½¬ RGB
            im = im.transpose((0, 3, 1, 2))  # BHWC â†’ BCHW, (n, 3, h, w)
            im = np.ascontiguousarray(im)  # ä¿è¯å†…å­˜è¿ç»­
            im = torch.from_numpy(im)

        im = im.to(self.device)
        im = im.half() if self.model.fp16 else im.float()  # uint8 â†’ fp16/32
        if not_tensor:
            im /= 255  # åƒç´ å½’ä¸€åŒ–åˆ° [0, 1]
        return im

    def inference(self, im: torch.Tensor, *args, **kwargs):
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹å’Œå‚æ•°å¯¹å›¾åƒè¿›è¡Œæ¨ç†ã€‚"""
        visualize = (
            increment_path(self.save_dir / Path(self.batch[0][0]).stem, mkdir=True)
            if self.args.visualize and (not self.source_type.tensor)
            else False
        )
        return self.model(im, augment=self.args.augment, visualize=visualize, embed=self.args.embed, *args, **kwargs)

    def pre_transform(self, im: list[np.ndarray]) -> list[np.ndarray]:
        """
        åœ¨æ¨ç†å‰å¯¹è¾“å…¥å›¾åƒæ‰§è¡Œ LetterBox ç­‰å‡ ä½•é¢„å¤„ç†ã€‚

        å‚æ•°:
            im (list[np.ndarray]): å›¾åƒåˆ—è¡¨ï¼Œæ¯å¼ å½¢çŠ¶ä¸º (H, W, 3)ã€‚

        è¿”å›:
            (list[np.ndarray]): é¢„å¤„ç†åçš„å›¾åƒåˆ—è¡¨ã€‚
        """
        same_shapes = len({x.shape for x in im}) == 1
        letterbox = LetterBox(
            self.imgsz,
            auto=same_shapes
            and self.args.rect
            and (self.model.pt or (getattr(self.model, "dynamic", False) and not self.model.imx)),
            stride=self.model.stride,
        )
        return [letterbox(image=x) for x in im]
    #------æ›¿æ¢å‡½æ•°å†…å®¹ï¼Œå¢åŠ  ROI æå–é€»è¾‘
    def postprocess(self, preds, img, orig_imgs):
        """
        1) NMS
        2) å°†æ¯å¼ å›¾ç‰‡çš„é¢„æµ‹æ„é€ æˆ Results å¯¹è±¡
        3) æå–è£…ç”²æ¿ ROIï¼ˆä¼˜å…ˆ OBB æ—‹è½¬è£å‰ªï¼Œé€€åŒ–ä¸º xyxy è£å‰ªï¼‰
        4) å°† ROI é™„åŠ åˆ°æ¯ä¸ª Resultsï¼ˆr.armor_roisï¼‰å¹¶ç¼“å­˜åˆ° self.armor_rois
        """
        # 1) NMSï¼ˆä¿æŒä¸ä½ å½“å‰æµç¨‹ä¸€è‡´ï¼‰
        preds = ops.non_max_suppression(preds,
                                        self.args.conf,
                                        self.args.iou,
                                        agnostic=self.args.agnostic_nms,
                                        max_det=self.args.max_det)

        results = []
        self.armor_rois.clear()  # æ¸…ç©ºå…¨å±€ç¼“å­˜

        # åç§°è¡¨ï¼ˆå¯èƒ½åœ¨ AutoBackend ä¸Šï¼‰
        names = getattr(self.model, 'names', None)
        # å…è®¸ç”¨ --armor-classes è‡ªå®šä¹‰ï¼›å¦åˆ™ç”¨é»˜è®¤å€™é€‰ï¼ˆæŒ‰ä½ ç°æœ‰å·¥ç¨‹ä¹ æƒ¯æ”¹ï¼‰
        armor_name_candidates = set(getattr(self.args, 'armor_classes', ['armor', 'armor_plate', 'plate']))

        for i, det in enumerate(preds):
            # å–åŸå›¾
            im0 = orig_imgs[i].copy() if isinstance(orig_imgs, list) else orig_imgs.copy()
            H, W = im0.shape[:2]

            # 2) scale å›åŸå›¾åæ ‡ï¼ˆå…ˆå¯¹å¸¸è§„ xyxyï¼‰
            if len(det):
                det[:, :4] = ops.scale_boxes(img.shape[2:], det[:, :4], im0.shape).round()

            # 3) ç»„è£… Resultsï¼ˆå…ˆæ”¾å¸¸è§„ boxes/probsï¼›masks è§†ä»»åŠ¡è€Œå®šï¼‰
            r = Results(
                path=None,
                boxes=det[:, :6] if len(det) else det,  # å…¼å®¹ boxesï¼ˆxyxy, conf, clsï¼‰
                masks=None,
                probs=None,
                names=names,
                orig_img=im0
            )

            # 4) ROI æå–ï¼ˆä¼˜å…ˆ OBBï¼›è‹¥æ—  OBB åˆ™ç”¨ xyxyï¼‰
            rois = []

            # 4.1 å…ˆå°è¯•ä» r æˆ– det ä¸­æ‰¾åˆ° OBB ä¿¡æ¯ï¼ˆä¸åŒåˆ†æ”¯å­—æ®µåä¸å®Œå…¨ä¸€è‡´ï¼‰
            # å¸¸è§ä½ç½®ï¼šr.obb / r.boxes.rboxes / det.obb / det.rboxes / è‡ªå®šä¹‰ attr
            obb_array = None
            # ä» r.boxes ä¸‹æ¸¸ç»“æ„å°è¯•
            if hasattr(r, 'boxes') and r.boxes is not None:
                # ä¸€äº›å®ç°æŠŠ OBB æ”¾åœ¨ r.boxes.rboxes æˆ– r.boxes.obb
                for key in ('rboxes', 'obb', 'xywhr', 'xywht'):
                    if hasattr(r.boxes, key):
                        obb_array = getattr(r.boxes, key, None)
                        if obb_array is not None:
                            try:
                                obb_array = np.asarray(obb_array, dtype=np.float32)
                            except Exception:
                                obb_array = None
                        if obb_array is not None:
                            break

            # 4.2 éå†æ¯ä¸ªæ£€æµ‹ï¼ŒåŒ¹é…è£…ç”²æ¿ç±»åˆ«
            if len(det):
                for j in range(det.shape[0]):
                    x1, y1, x2, y2, conf, cls = det[j].tolist()
                    cls = int(cls)
                    cls_name = (names[cls] if names and cls in range(len(names)) else str(cls))

                    # è‹¥æŒ‡å®šäº†è£…ç”²æ¿ç±»åˆ«åæ‰æå– ROI
                    if cls_name not in armor_name_candidates:
                        continue

                    # ä¼˜å…ˆç”¨ OBB çš„ç¬¬ j ä¸ªæ¡†
                    roi = None
                    if obb_array is not None and j < len(obb_array):
                        # å…¼å®¹è‹¥ obb ä¸º [cx,cy,w,h,theta] æˆ– [x,y,w,h,theta]ï¼›å‡å®šä¸ºåƒç´ åæ ‡ + å¼§åº¦
                        cx, cy, w, h, theta = obb_array[j][:5]
                        # å®¹é”™ï¼šè‹¥æ˜¯å½’ä¸€åŒ–ï¼Œæ”¾å¤§å›åƒç´ 
                        if max(cx, cy, w, h) <= 1.5:  # ç²—ç•¥åˆ¤æ–­
                            cx, cy, w, h = cx * W, cy * H, w * W, h * H
                        quad = self._rbox_to_quad(cx, cy, w, h, theta)
                        try:
                            roi = self._crop_quad(im0, quad, out_size=(64, 64))
                        except Exception:
                            roi = None

                    # å›é€€ï¼šè‹¥æ²¡æœ‰ OBB æˆ–è£å‰ªå¤±è´¥ï¼Œåˆ™ç”¨è½´å¯¹é½è£å‰ª
                    if roi is None:
                        xi1, yi1 = max(0, int(x1)), max(0, int(y1))
                        xi2, yi2 = min(W, int(x2)), min(H, int(y2))
                        if (xi2 - xi1) > 1 and (yi2 - yi1) > 1:
                            roi = im0[yi1:yi2, xi1:xi2].copy()

                    if roi is not None and roi.size > 0:
                        rois.append(roi)

            # æŠŠ ROI æ”¾è¿›å•å¼ ç»“æœï¼›åŒæ—¶ç¼“å­˜åˆ° predictor ä¸Šï¼ˆä¾›å¤–éƒ¨è¯»å–ï¼‰
            r.armor_rois = rois
            results.append(r)
            self.armor_rois.append(rois)

        return results
    def setup_source(self, source):
        """
        è®¾ç½®è¾“å…¥æºåŠæ¨ç†æ¨¡å¼ã€‚

        å‚æ•°:
            source (str | Path | list[str] | list[Path] | list[np.ndarray] | np.ndarray | torch.Tensor):
                æ¨ç†è¾“å…¥æºï¼Œå¯ä»¥æ˜¯å•å¼ å›¾åƒã€æ–‡ä»¶å¤¹ã€è§†é¢‘è·¯å¾„ã€æµåª’ä½“ URL æˆ–å¼ é‡ã€‚
        """
        # æ£€æŸ¥è¾“å…¥å›¾åƒå°ºå¯¸æ˜¯å¦ç¬¦åˆæ¨¡å‹è¦æ±‚
        self.imgsz = check_imgsz(self.args.imgsz, stride=self.model.stride, min_dim=2)
        # æ ¹æ®è¾“å…¥æºåˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.dataset = load_inference_source(
            source=source,
            batch=self.args.batch,
            vid_stride=self.args.vid_stride,
            buffer=self.args.stream_buffer,
            channels=getattr(self.model, "ch", 3),
        )
        # è®°å½•è¾“å…¥æºç±»å‹
        self.source_type = self.dataset.source_type

        # åˆ¤æ–­æ˜¯å¦ä¸ºé•¿åºåˆ—(ä¾‹å¦‚è§†é¢‘æˆ–å¤§å‹æ•°æ®é›†)
        long_sequence = (
            self.source_type.stream
            or self.source_type.screenshot
            or len(self.dataset) > 1000  # è¶…è¿‡ 1000 å¼ å›¾åƒ
            or any(getattr(self.dataset, "video_flag", [False]))  # æ˜¯å¦ä¸ºè§†é¢‘æµ
        )
        if long_sequence:
            import torchvision  # å»¶è¿Ÿå¯¼å…¥ï¼Œè§¦å‘ torchvision çš„ NMS å®ç°

            if not getattr(self, "stream", True):  # è‹¥ä¸æ˜¯æµå¼æ¨¡å¼åˆ™å‘å‡ºè­¦å‘Š
                LOGGER.warning(STREAM_WARNING)
        # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
        self.vid_writer = {}

    @smart_inference_mode()
    def stream_inference(self, source=None, model=None, *args, **kwargs):
        """
        åœ¨æµåª’ä½“æˆ–è§†é¢‘ä¸Šæ‰§è¡Œå®æ—¶æ¨ç†ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶ã€‚

        å‚æ•°:
            source: è¾“å…¥æº(å›¾åƒã€è§†é¢‘ã€æµç­‰)ã€‚
            model: è¦åŠ è½½æˆ–ä½¿ç”¨çš„æ¨¡å‹ã€‚
            *args, **kwargs: é¢å¤–æ¨ç†å‚æ•°ã€‚

        è¿”å›:
            (generator): é€å¸§è¾“å‡ºçš„æ¨ç†ç»“æœå¯¹è±¡ã€‚
        """
        if self.args.verbose:
            LOGGER.info("")

        # æ¨¡å‹åˆå§‹åŒ–
        if not self.model:
            self.setup_model(model)

        # ä½¿ç”¨çº¿ç¨‹é”ä»¥ä¿è¯å¤šçº¿ç¨‹ç¯å¢ƒä¸‹æ¨ç†å®‰å…¨
        with self._lock:
            # æ¯æ¬¡è°ƒç”¨ predict æ—¶é‡æ–°è®¾ç½®è¾“å…¥æº
            self.setup_source(source if source is not None else self.args.source)

            # è‹¥å¼€å¯ä¿å­˜é€‰é¡¹ï¼Œåˆ™åˆ›å»ºç»“æœè¾“å‡ºæ–‡ä»¶å¤¹
            if self.args.save or self.args.save_txt:
                (self.save_dir / "labels" if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)

            # æ¨¡å‹é¢„çƒ­(æé«˜ç¬¬ä¸€æ¬¡æ¨ç†é€Ÿåº¦)
            if not self.done_warmup:
                self.model.warmup(
                    imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, self.model.ch, *self.imgsz)
                )
                self.done_warmup = True

            # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
            self.seen, self.windows, self.batch = 0, [], None
            profilers = (
                ops.Profile(device=self.device),  # é¢„å¤„ç†è®¡æ—¶
                ops.Profile(device=self.device),  # æ¨ç†è®¡æ—¶
                ops.Profile(device=self.device),  # åå¤„ç†è®¡æ—¶
            )

            # æ‰§è¡Œæ¨ç†èµ·å§‹å›è°ƒ
            self.run_callbacks("on_predict_start")

            # éå†è¾“å…¥æ•°æ®é›†(é€æ‰¹æ¨ç†)
            for self.batch in self.dataset:
                self.run_callbacks("on_predict_batch_start")
                paths, im0s, s = self.batch

                # --------- å›¾åƒé¢„å¤„ç†é˜¶æ®µ ---------
                with profilers[0]:
                    im = self.preprocess(im0s)

                # --------- æ¨¡å‹æ¨ç†é˜¶æ®µ ---------
                with profilers[1]:
                    preds = self.inference(im, *args, **kwargs)
                    if self.args.embed:
                        # å¦‚æœæ˜¯ç‰¹å¾åµŒå…¥ä»»åŠ¡ï¼Œåˆ™ç›´æ¥è¾“å‡ºå¼ é‡ç»“æœ
                        yield from [preds] if isinstance(preds, torch.Tensor) else preds
                        continue

                # ---------åå¤„ç†é˜¶æ®µ ---------
                with profilers[2]:
                    self.results = self.postprocess(preds, im, im0s)
                self.run_callbacks("on_predict_postprocess_end")

                # --------- ç»“æœå¤„ç†ä¸ä¿å­˜ ---------
                n = len(im0s)
                try:
                    for i in range(n):
                        self.seen += 1
                        self.results[i].speed = {
                            "preprocess": profilers[0].dt * 1e3 / n,
                            "inference": profilers[1].dt * 1e3 / n,
                            "postprocess": profilers[2].dt * 1e3 / n,
                        }
                        # æ§åˆ¶å°æ‰“å°ã€ä¿å­˜æˆ–æ˜¾ç¤ºæ¨ç†ç»“æœ
                        if self.args.verbose or self.args.save or self.args.save_txt or self.args.show:
                            s[i] += self.write_results(i, Path(paths[i]), im, s)
                except StopIteration:
                    break

                # æ‰¹æ¬¡ç»“æœæ‰“å°
                if self.args.verbose:
                    LOGGER.info("\n".join(s))

                self.run_callbacks("on_predict_batch_end")
                yield from self.results  # å°†ç»“æœè¿”å›ç»™ä¸Šå±‚è°ƒç”¨è€…

        # ---------  æ¨ç†å®Œæˆåé‡Šæ”¾èµ„æº ---------
        for v in self.vid_writer.values():
            if isinstance(v, cv2.VideoWriter):
                v.release()

        # è‹¥å¼€å¯çª—å£æ˜¾ç¤ºï¼Œåˆ™å…³é—­æ‰€æœ‰çª—å£
        if self.args.show:
            cv2.destroyAllWindows()

        # ---------  æœ€ç»ˆç»“æœä¿¡æ¯æ‰“å° ---------
        if self.args.verbose and self.seen:
            t = tuple(x.t / self.seen * 1e3 for x in profilers)
            LOGGER.info(
                f"Speed: %.1fms preprocess, %.1fms inference, %.1fms postprocess per image at shape "
                f"{(min(self.args.batch, self.seen), getattr(self.model, 'ch', 3), *im.shape[2:])}" % t
            )

        # ä¿å­˜æ ‡ç­¾ä¸æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        if self.args.save or self.args.save_txt or self.args.save_crop:
            nl = len(list(self.save_dir.glob("labels/*.txt")))
            s = f"\n{nl} ä¸ªæ ‡ç­¾æ–‡ä»¶å·²ä¿å­˜è‡³ {self.save_dir / 'labels'}" if self.args.save_txt else ""
            LOGGER.info(f"ç»“æœå·²ä¿å­˜åˆ° {colorstr('bold', self.save_dir)}{s}")
        self.run_callbacks("on_predict_end")

    def setup_model(self, model, verbose: bool = True):
        """
        åˆå§‹åŒ– YOLO æ¨¡å‹ï¼Œå¹¶å°†å…¶è®¾ç½®ä¸ºæ¨ç†(è¯„ä¼°)æ¨¡å¼ã€‚

        å‚æ•°:
            model (str | Path | torch.nn.Module, å¯é€‰): è¦åŠ è½½æˆ–ä½¿ç”¨çš„æ¨¡å‹ã€‚
            verbose (bool): æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯ã€‚
        """
        self.model = AutoBackend(
            model=model or self.args.model,  # åŠ è½½æ¨¡å‹æ–‡ä»¶æˆ–ä¼ å…¥çš„æ¨¡å‹å¯¹è±¡
            device=select_device(self.args.device, verbose=verbose),  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
            dnn=self.args.dnn,       # æ˜¯å¦ä½¿ç”¨ OpenCV DNN æ¨¡å¼
            data=self.args.data,     # æ•°æ®é…ç½®
            fp16=self.args.half,     # åŠç²¾åº¦æ¨¡å¼
            fuse=True,               # æ¨¡å‹èåˆä¼˜åŒ–
            verbose=verbose,
        )

        # æ›´æ–°è®¾å¤‡ä¸ç²¾åº¦è®¾ç½®
        self.device = self.model.device
        self.args.half = self.model.fp16

        # è‹¥æ¨¡å‹ä¸­ä¿å­˜æœ‰å›¾åƒå°ºå¯¸å…ƒæ•°æ®ï¼Œåˆ™æ²¿ç”¨
        if hasattr(self.model, "imgsz") and not getattr(self.model, "dynamic", False):
            self.args.imgsz = self.model.imgsz

        # åˆ‡æ¢ä¸ºæ¨ç†æ¨¡å¼
        self.model.eval()

        # å°è¯•ç¼–è¯‘æ¨¡å‹(è‹¥è®¾å¤‡æ”¯æŒ)
        self.model = attempt_compile(self.model, device=self.device, mode=self.args.compile)

    def write_results(self, i: int, p: Path, im: torch.Tensor, s: list[str]) -> str:
        """
        å°†æ¨ç†ç»“æœå†™å…¥æ–‡ä»¶æˆ–ç›®å½•ã€‚

        å‚æ•°:
            i (int): å½“å‰æ‰¹æ¬¡ä¸­å›¾åƒçš„ç´¢å¼•ã€‚
            p (Path): å½“å‰å›¾åƒçš„è·¯å¾„ã€‚
            im (torch.Tensor): é¢„å¤„ç†åçš„å›¾åƒå¼ é‡ã€‚
            s (list[str]): æ‰¹æ¬¡çŠ¶æ€ä¿¡æ¯å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

        è¿”å›:
            (str): åŒ…å«ç»“æœä¿¡æ¯çš„å­—ç¬¦ä¸²(ç”¨äºæ§åˆ¶å°è¾“å‡º)ã€‚
        """
        string = ""  # è¾“å‡ºå­—ç¬¦ä¸²åˆå§‹åŒ–

        # è‹¥å›¾åƒç¼ºå°‘æ‰¹æ¬¡ç»´åº¦åˆ™æ·»åŠ 
        if len(im.shape) == 3:
            im = im[None]

        # åˆ¤æ–­è¾“å…¥æºç±»å‹(è§†é¢‘æµã€å•å›¾åƒã€å¼ é‡è¾“å…¥ç­‰)
        if self.source_type.stream or self.source_type.from_img or self.source_type.tensor:
            string += f"{i}: "
            frame = self.dataset.count
        else:
            # ä»çŠ¶æ€å­—ç¬¦ä¸²ä¸­æå–å¸§å·
            match = re.search(r"frame (\d+)/", s[i])
            frame = int(match[1]) if match else None

        # ç”Ÿæˆæ–‡æœ¬ç»“æœè·¯å¾„(labels)
        self.txt_path = self.save_dir / "labels" / (p.stem + ("" if self.dataset.mode == "image" else f"_{frame}"))
        string += "{:g}x{:g} ".format(*im.shape[2:])

        # è·å–å½“å‰æ¨ç†ç»“æœå¯¹è±¡
        result = self.results[i]
        # === ä¸¤è¡Œå¯¹æ¥ï¼šå°† ROI ä¸¢ç»™æ•°å­—åˆ†ç±»å™¨ ===
        if getattr(result, "armor_rois", None):
            digits, scores, _ = classify_rois(
                result.armor_rois,
                weights="digit_classifier.pt",
                device=self.device,
                conf_thr=0.6,  # ä½äº 0.6 çš„è®¤ä¸ºæ˜¯â€œè¯†åˆ«å¤±è´¥â€
            )
            result.digits, result.digit_scores = digits, scores


        # ----------- å¯è§†åŒ–éƒ¨åˆ† -----------
        if self.args.save or self.args.show:
            self.plotted_img = result.plot(
                line_width=self.args.line_width,
                boxes=self.args.show_boxes,
                conf=self.args.show_conf,
                labels=self.args.show_labels,
                im_gpu=None if self.args.retina_masks else im[i],
            )

            # === æŠŠæ•°å­—å åˆ°è£…ç”²æ¿æ¡†ä¸Š ===
            if getattr(result, "digits", None) and getattr(result, "boxes", None) and len(result.boxes):
                import cv2
                armor_name_candidates = set(getattr(self.args, "armor_classes", ["armor", "armor_plate", "plate"]))
                names = getattr(self.model, "names", None)

                xyxy = result.boxes.xyxy.cpu().numpy()
                clss = result.boxes.cls.cpu().numpy().astype(int)

                k = 0  # digits çš„æ¸¸æ ‡ï¼Œåªç»™è£…ç”²æ¿ç±»æ¡†å†™æ•°å­—
                for j in range(len(xyxy)):
                    cls_id = clss[j]
                    cls_name = (names[cls_id] if names and cls_id < len(names) else str(cls_id))
                    if cls_name not in armor_name_candidates:
                        continue
                    if k >= len(result.digits):
                        break

                    x1, y1, x2, y2 = map(int, xyxy[j])
                    txt = f"{result.digits[k]} ({result.digit_scores[k]:.2f})"
                    org = (x1, max(0, y1 - 6))
                    cv2.putText(self.plotted_img, txt, org, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, cv2.LINE_AA)
                    k += 1
            result.save_dir = self.save_dir.__str__()  # ä¸ºå…¶ä»–æ¨¡å—æä¾›è·¯å¾„å¼•ç”¨
            string += f"{result.verbose()}{result.speed['inference']:.1f}ms"
                # ----------- å¯è§†åŒ–éƒ¨åˆ† -----------
            if self.args.save or self.args.show:
                self.plotted_img = result.plot(
                    line_width=self.args.line_width,   # ç»˜åˆ¶çº¿æ¡å®½åº¦
                    boxes=self.args.show_boxes,        # æ˜¯å¦æ˜¾ç¤ºè¾¹ç•Œæ¡†
                    conf=self.args.show_conf,          # æ˜¯å¦æ˜¾ç¤ºç½®ä¿¡åº¦
                    labels=self.args.show_labels,      # æ˜¯å¦æ˜¾ç¤ºæ ‡ç­¾
                    im_gpu=None if self.args.retina_masks else im[i],
                )

        # ----------- ç»“æœä¿å­˜éƒ¨åˆ† -----------
        if self.args.save_txt:
            result.save_txt(f"{self.txt_path}.txt", save_conf=self.args.save_conf)
        if self.args.save_crop:
            result.save_crop(save_dir=self.save_dir / "crops", file_name=self.txt_path.stem)
        if self.args.show:
            self.show(str(p))
        if self.args.save:
            self.save_predicted_images(self.save_dir / p.name, frame)

        return string

    def save_predicted_images(self, save_path: Path, frame: int = 0):
        """
        å°†è§†é¢‘æˆ–å›¾åƒçš„æ¨ç†ç»“æœä¿å­˜ä¸º mp4 æˆ– jpg æ–‡ä»¶ã€‚

        å‚æ•°:
            save_path (Path): ç»“æœä¿å­˜è·¯å¾„ã€‚
            frame (int): å½“å‰å¸§ç¼–å·(ä»…è§†é¢‘æ¨¡å¼ä¸‹æœ‰æ•ˆ)ã€‚
        """
        im = self.plotted_img  # ç»˜åˆ¶åçš„å›¾åƒ

        # ----------- è§†é¢‘æˆ–æµåª’ä½“æ¨¡å¼ -----------
        if self.dataset.mode in {"stream", "video"}:
            fps = self.dataset.fps if self.dataset.mode == "video" else 30
            frames_path = self.save_dir / f"{save_path.stem}_frames"  # å•ç‹¬å­˜å‚¨å¸§å›¾åƒ

            # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
            if save_path not in self.vid_writer:
                if self.args.save_frames:
                    Path(frames_path).mkdir(parents=True, exist_ok=True)
                # ä¸åŒç³»ç»Ÿä¸‹çš„ç¼–ç å™¨é€‰æ‹©
                suffix, fourcc = (".mp4", "avc1") if MACOS else (".avi", "WMV2") if WINDOWS else (".avi", "MJPG")
                self.vid_writer[save_path] = cv2.VideoWriter(
                    filename=str(Path(save_path).with_suffix(suffix)),
                    fourcc=cv2.VideoWriter_fourcc(*fourcc),
                    fps=fps,  # å¿…é¡»ä¸ºæ•´æ•°ï¼Œå¦åˆ™éƒ¨åˆ†ç¼–ç å™¨æŠ¥é”™
                    frameSize=(im.shape[1], im.shape[0]),  # (å®½, é«˜)
                )

            # å†™å…¥è§†é¢‘å¸§
            self.vid_writer[save_path].write(im)
            if self.args.save_frames:
                cv2.imwrite(f"{frames_path}/{save_path.stem}_{frame}.jpg", im)

        # ----------- é™æ€å›¾åƒæ¨¡å¼ -----------
        else:
            cv2.imwrite(str(save_path.with_suffix(".jpg")), im)  # ä¿å­˜ä¸º JPG æ ¼å¼(å…¼å®¹æ€§æœ€å¥½)

    def show(self, p: str = ""):
        """
        åœ¨çª—å£ä¸­æ˜¾ç¤ºå›¾åƒã€‚

        å‚æ•°:
            p (str): çª—å£æ ‡é¢˜æˆ–å›¾åƒè·¯å¾„(ç”¨äºæ ‡è¯†)ã€‚
        """
        im = self.plotted_img
        if platform.system() == "Linux" and p not in self.windows:
            self.windows.append(p)
            cv2.namedWindow(p, cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # å…è®¸çª—å£ç¼©æ”¾(Linux ä¸“ç”¨)
            cv2.resizeWindow(p, im.shape[1], im.shape[0])  # (å®½, é«˜)
        cv2.imshow(p, im)

        # æŒ‰ä¸‹ q é”®å¯é€€å‡º(å›¾åƒæ¨¡å¼ä¸‹å»¶è¿Ÿ 300msï¼Œè§†é¢‘æµæ¨¡å¼ä¸‹ 1ms)
        if cv2.waitKey(300 if self.dataset.mode == "image" else 1) & 0xFF == ord("q"):
            raise StopIteration

    def run_callbacks(self, event: str):
        """
        è¿è¡Œç‰¹å®šäº‹ä»¶çš„æ‰€æœ‰å›è°ƒå‡½æ•°ã€‚

        å‚æ•°:
            event (str): äº‹ä»¶åç§°(å¦‚ "on_predict_start"ã€"on_predict_end")ã€‚
        """
        for callback in self.callbacks.get(event, []):
            callback(self)

    def add_callback(self, event: str, func: callable):
        """
        ä¸ºç‰¹å®šäº‹ä»¶æ·»åŠ æ–°çš„å›è°ƒå‡½æ•°ã€‚

        å‚æ•°:
            event (str): äº‹ä»¶åç§°ã€‚
            func (callable): å›è°ƒå‡½æ•°ã€‚
        """
        self.callbacks[event].append(func)
