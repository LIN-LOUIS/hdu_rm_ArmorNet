# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

# =============================
# æœ¬æ–‡ä»¶ä¸ºç”¨æˆ·æä¾›çš„ dataset.py ç‰‡æ®µçš„ã€Œä¸­æ–‡æ³¨é‡Šç‰ˆã€ã€‚
# ç›®æ ‡ï¼šåœ¨ä¸æ”¹å˜ä»»ä½•åŠŸèƒ½ä¸é€»è¾‘çš„å‰æä¸‹ï¼Œè¡¥å……ä¸­æ–‡è¡Œå†…æ³¨é‡Šï¼Œ
#      ä¾¿äºå¿«é€Ÿç†è§£ YOLO/OBB/å¤šæ¨¡æ€/grounding æ•°æ®é›†ç®¡çº¿çš„å®ç°ç»†èŠ‚ã€‚
# è¯´æ˜ï¼šä»…å¢åŠ æ³¨é‡Šä¸æå°‘é‡æ’ç‰ˆ(ç©ºè¡Œ)ï¼Œä¸æ”¹åŠ¨å˜é‡åä¸è¯­ä¹‰ã€‚
# =============================

from __future__ import annotations

import json
from collections import defaultdict
from itertools import repeat
from multiprocessing.pool import ThreadPool
from pathlib import Path
from typing import Any

import cv2
import numpy as np
import torch
from PIL import Image
from torch.utils.data import ConcatDataset

from ultralytics.utils import LOCAL_RANK, LOGGER, NUM_THREADS, TQDM, colorstr
from ultralytics.utils.instance import Instances
from ultralytics.utils.ops import resample_segments, segments2boxes
from ultralytics.utils.torch_utils import TORCHVISION_0_18

from .augment import (
    Compose,
    Format,
    LetterBox,
    RandomLoadText,
    classify_augmentations,
    classify_transforms,
    v8_transforms,
)
from .base import BaseDataset
from .converter import merge_multi_segment
from .utils import (
    HELP_URL,
    check_file_speeds,
    get_hash,
    img2label_paths,
    load_dataset_cache_file,
    save_dataset_cache_file,
    verify_image,
    verify_image_label,
)

# Ultralytics dataset *.cache çš„ç‰ˆæœ¬å·ï¼Œéœ€ä¸å½“å‰ä»£ç æœŸæœ›çš„ç‰ˆæœ¬æ¯”å¯¹
DATASET_CACHE_VERSION = "1.0.3"


class YOLODataset(BaseDataset):
    """
    ç”¨äºåŠ è½½ YOLO æ ‡æ³¨(æ£€æµ‹/åˆ†å‰²/å…³é”®ç‚¹/OBB)çš„é€šç”¨æ•°æ®é›†ç±»ã€‚

    - use_segments: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰/å®ä¾‹åˆ†å‰²å¤šè¾¹å½¢(segments)
    - use_keypoints: æ˜¯å¦ä½¿ç”¨å…³é”®ç‚¹(pose)
    - use_obb: æ˜¯å¦ä½¿ç”¨å®šå‘æ¡†(oriented bounding box)
    - data: æ•°æ®é›†é…ç½®å­—å…¸(é€šå¸¸æ¥è‡ª data.yaml)
    """

    def __init__(self, *args, data: dict | None = None, task: str = "detect", **kwargs):
        """
        Args:
            data: æ•°æ®é›†é…ç½®(å«ç±»åˆ«åã€å…³é”®ç‚¹å½¢çŠ¶ç­‰)
            task: ä»»åŠ¡ç±»å‹ï¼š'detect' | 'segment' | 'pose' | 'obb'
        """
        # ä¸‰ä¸ªå¸ƒå°”å¼€å…³æ ¹æ® task å†³å®šä¸åŒçš„è¯»å–/æ‰“åŒ…é€»è¾‘
        self.use_segments = task == "segment"
        self.use_keypoints = task == "pose"
        self.use_obb = task == "obb"
        self.data = data
        # åˆ†å‰²ä¸å…³é”®ç‚¹ä¸èƒ½åŒæ—¶ä¸º True
        assert not (self.use_segments and self.use_keypoints), "Can not use both segments and keypoints."
        # è°ƒç”¨çˆ¶ç±»ï¼Œchannels ä» data['channels'] è¯»å–(é»˜è®¤ 3)
        super().__init__(*args, channels=self.data.get("channels", 3), **kwargs)

    def cache_labels(self, path: Path = Path("./labels.cache")) -> dict:
        """
        æ‰«æä¸æ ¡éªŒå›¾åƒä¸æ ‡ç­¾ï¼Œç”Ÿæˆç¼“å­˜(å« shapes/hash/ç»Ÿè®¡ä¿¡æ¯ç­‰)ã€‚
        - ä½¿ç”¨ ThreadPool å¹¶è¡Œè°ƒç”¨ verify_image_label æé€Ÿã€‚
        - å°†æ¯å¼ å›¾çš„ cls/bboxes/segments/keypoints ç­‰ç»„ç»‡ä¸ºæ ‡å‡†æ ¼å¼ã€‚
        - æœ€ç»ˆå†™å…¥ *.cache ä»¥ä¾¿ä¸‹æ¬¡å¿«é€ŸåŠ è½½ã€‚
        """
        x = {"labels": []}
        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # ç»Ÿè®¡ï¼šmissing / found / empty / corrupt
        desc = f"{self.prefix}Scanning {path.parent / path.stem}..."
        total = len(self.im_files)
        nkpt, ndim = self.data.get("kpt_shape", (0, 0))
        if self.use_keypoints and (nkpt <= 0 or ndim not in {2, 3}):
            # å…³é”®ç‚¹ä»»åŠ¡è¦æ±‚ data.yaml ä¸­ç»™å®š kpt_shape=[K, D]
            raise ValueError(
                "'kpt_shape' in data.yaml missing or incorrect. Should be a list with [number of "
                "keypoints, number of dims (2 for x,y or 3 for x,y,visible)], i.e. 'kpt_shape: [17, 3]'"
            )
        # å¹¶è¡Œæ ¡éªŒæ¯å¼ å›¾åƒä¸å…¶ label æ–‡æœ¬
        with ThreadPool(NUM_THREADS) as pool:
            results = pool.imap(
                func=verify_image_label,
                iterable=zip(
                    self.im_files,
                    self.label_files,
                    repeat(self.prefix),
                    repeat(self.use_keypoints),
                    repeat(len(self.data["names"])),
                    repeat(nkpt),
                    repeat(ndim),
                    repeat(self.single_cls),
                ),
            )
            pbar = TQDM(results, desc=desc, total=total)
            for im_file, lb, shape, segments, keypoint, nm_f, nf_f, ne_f, nc_f, msg in pbar:
                nm += nm_f; nf += nf_f; ne += ne_f; nc += nc_f
                if im_file:
                    # å°†ä¸€å¼ å›¾çš„æ ‡æ³¨æ•´ç†ä¸ºç»Ÿä¸€ dict ç»“æ„
                    x["labels"].append(
                        {
                            "im_file": im_file,
                            "shape": shape,
                            "cls": lb[:, 0:1],      # [n,1]
                            "bboxes": lb[:, 1:],     # [n,4]ï¼Œxywh æ ¼å¼(å½’ä¸€åŒ–)
                            "segments": segments,
                            "keypoints": keypoint,
                            "normalized": True,
                            "bbox_format": "xywh",
                        }
                    )
                if msg:
                    msgs.append(msg)
                pbar.desc = f"{desc} {nf} images, {nm + ne} backgrounds, {nc} corrupt"
            pbar.close()

        if msgs:
            LOGGER.info("\n".join(msgs))
        if nf == 0:
            LOGGER.warning(f"{self.prefix}No labels found in {path}. {HELP_URL}")
        # è®¡ç®— hash ç”¨äºä¸ç¼“å­˜å¯¹æ¯”
        x["hash"] = get_hash(self.label_files + self.im_files)
        x["results"] = nf, nm, ne, nc, len(self.im_files)
        x["msgs"] = msgs
        save_dataset_cache_file(self.prefix, path, x, DATASET_CACHE_VERSION)
        return x

    def get_labels(self) -> list[dict]:
        """
        åŠ è½½/ç”Ÿæˆæ ‡ç­¾ç¼“å­˜ï¼Œå¹¶åšä¸€è‡´æ€§æ£€æŸ¥ä¸æç¤ºã€‚
        - è‹¥ç¼“å­˜å­˜åœ¨ä¸” hash/version åŒ¹é…ï¼Œåˆ™ç›´æ¥ç”¨ç¼“å­˜ã€‚
        - å¦åˆ™é‡æ–°æ‰«æï¼Œç”Ÿæˆç¼“å­˜ã€‚
        - æ£€æŸ¥ boxes ä¸ segments æ•°é‡ä¸åŒ¹é…æ—¶ç»™å‡ºè­¦å‘Šå¹¶ä¸¢å¼ƒ segmentsã€‚
        """
        self.label_files = img2label_paths(self.im_files)
        cache_path = Path(self.label_files[0]).parent.with_suffix(".cache")
        try:
            cache, exists = load_dataset_cache_file(cache_path), True
            assert cache["version"] == DATASET_CACHE_VERSION
            assert cache["hash"] == get_hash(self.label_files + self.im_files)
        except (FileNotFoundError, AssertionError, AttributeError, ModuleNotFoundError):
            cache, exists = self.cache_labels(cache_path), False

        # è¿›åº¦æ¡æ˜¾ç¤ºç¼“å­˜æ‘˜è¦
        nf, nm, ne, nc, n = cache.pop("results")
        if exists and LOCAL_RANK in {-1, 0}:
            d = f"Scanning {cache_path}... {nf} images, {nm + ne} backgrounds, {nc} corrupt"
            TQDM(None, desc=self.prefix + d, total=n, initial=n)
            if cache["msgs"]:
                LOGGER.info("\n".join(cache["msgs"]))

        # æå– labels åˆ—è¡¨å¹¶æ›´æ–° im_files
        [cache.pop(k) for k in ("hash", "version", "msgs")]
        labels = cache["labels"]
        if not labels:
            raise RuntimeError(
                f"No valid images found in {cache_path}. Images with incorrectly formatted labels are ignored. {HELP_URL}"
            )
        self.im_files = [lb["im_file"] for lb in labels]

        # ç»Ÿè®¡æ˜¯å¦ä¸ºçº¯ boxes æˆ–çº¯ segmentsï¼Œä»¥é¿å… detect/segment æ··åˆå¯¼è‡´ä¸ä¸€è‡´
        lengths = ((len(lb["cls"]), len(lb["bboxes"]), len(lb["segments"])) for lb in labels)
        len_cls, len_boxes, len_segments = (sum(x) for x in zip(*lengths))
        if len_segments and len_boxes != len_segments:
            LOGGER.warning(
                f"Box and segment counts should be equal, but got len(segments) = {len_segments}, "
                f"len(boxes) = {len_boxes}. To resolve this only boxes will be used and all segments will be removed. "
                "To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset."
            )
            for lb in labels:
                lb["segments"] = []
        if len_cls == 0:
            LOGGER.warning(f"Labels are missing or empty in {cache_path}, training may not work correctly. {HELP_URL}")
        return labels

    def build_transforms(self, hyp: dict | None = None) -> Compose:
        """
        æ„å»ºå›¾åƒå¢å¼º/é¢„å¤„ç†æµæ°´çº¿ã€‚
        - è®­ç»ƒæ—¶ï¼šä¾æ® mosaic/mixup/cutmix ç­‰è¶…å‚æ„é€  v8_transforms
        - éªŒè¯/æ¨ç†æ—¶ï¼šä»…åš LetterBox ä¸ Format
        - Format é‡Œä¼šç»Ÿä¸€ bbox æ ¼å¼ã€å½’ä¸€åŒ–ã€ä»¥åŠæ˜¯å¦å¯¼å‡º mask/keypoint/obb
        """
        if self.augment:
            hyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0
            hyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0
            hyp.cutmix = hyp.cutmix if self.augment and not self.rect else 0.0
            transforms = v8_transforms(self, self.imgsz, hyp)
        else:
            transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False)])
        transforms.append(
            Format(
                bbox_format="xywh",
                normalize=True,
                return_mask=self.use_segments,
                return_keypoint=self.use_keypoints,
                return_obb=self.use_obb,
                batch_idx=True,
                mask_ratio=hyp.mask_ratio,
                mask_overlap=hyp.overlap_mask,
                bgr=hyp.bgr if self.augment else 0.0,  # ä»…è®­ç»ƒé˜¶æ®µå½±å“
            )
        )
        return transforms

    def close_mosaic(self, hyp: dict) -> None:
        """åœ¨åæœŸè®­ç»ƒé˜¶æ®µå…³é—­ mosaic/copy_paste/mixup/cutmixï¼Œç¨³å®šæ”¶æ•›ã€‚"""
        hyp.mosaic = 0.0
        hyp.copy_paste = 0.0
        hyp.mixup = 0.0
        hyp.cutmix = 0.0
        self.transforms = self.build_transforms(hyp)

    def update_labels_info(self, label: dict) -> dict:
        """
        å°†åŸå§‹æ ‡ç­¾å­—å…¸è½¬æ¢ä¸º Instances å¯¹è±¡ä»¥ä¾¿åç»­ç»„ç½‘/æŸå¤±è®¡ç®—ã€‚
        - segments è‹¥å­˜åœ¨åˆ™è¿›è¡Œç­‰è·é‡é‡‡æ ·(é OBB æƒ…å†µä¸‹æ›´å¤šç‚¹æ•°)
        - æœ€ç»ˆç”Ÿæˆ ultralytics.utils.instance.Instances
        """
        bboxes = label.pop("bboxes")
        segments = label.pop("segments", [])
        keypoints = label.pop("keypoints", None)
        bbox_format = label.pop("bbox_format")
        normalized = label.pop("normalized")

        # OBB æƒ…å†µä¸‹ segment_resamples ç½®ä¸ºæ›´å°(ç‚¹æ•°æ›´å°‘ï¼Œé¿å…è¯¯å¤„ç† OBB)
        segment_resamples = 100 if self.use_obb else 1000
        if len(segments) > 0:
            # è‹¥åŸå§‹åˆ†å‰²ç‚¹æ•°æ¯”ç›®æ ‡ resamples å¤šï¼Œéœ€é‡æ–°æ’å€¼ä¿è¯ç­‰è·
            max_len = max(len(s) for s in segments)
            segment_resamples = (max_len + 1) if segment_resamples < max_len else segment_resamples
            # stack æˆå½¢çŠ¶ [num_instances, segment_resamples, 2]
            segments = np.stack(resample_segments(segments, n=segment_resamples), axis=0)
        else:
            segments = np.zeros((0, segment_resamples, 2), dtype=np.float32)
        label["instances"] = Instances(bboxes, segments, keypoints, bbox_format=bbox_format, normalized=normalized)
        return label

    @staticmethod
    def collate_fn(batch: list[dict]) -> dict:
        """
        DataLoader ç”¨çš„æ‹¼æ¥å‡½æ•°ï¼šå°†ä¸åŒæ ·æœ¬çš„å¼ é‡/åˆ—è¡¨æŒ‰é”®åˆå¹¶ä¸ºæ‰¹æ¬¡ã€‚
        - img/text_feats ç”¨ torch.stack
        - visuals ç”¨ pad_sequence(å¯å˜é•¿)
        - masks/keypoints/bboxes/cls/segments/obb ç”¨ cat
        - ç»´æŠ¤ batch_idxï¼Œä½¿å¾—æ¯ä¸ªç›®æ ‡çŸ¥é“è‡ªå·±æ¥è‡ªå“ªå¼ å›¾
        """
        new_batch = {}
        batch = [dict(sorted(b.items())) for b in batch]  # é”®æ’åºï¼Œä¿è¯ä¸€è‡´æ€§
        keys = batch[0].keys()
        values = list(zip(*[list(b.values()) for b in batch]))
        for i, k in enumerate(keys):
            value = values[i]
            if k in {"img", "text_feats"}:
                value = torch.stack(value, 0)
            elif k == "visuals":
                value = torch.nn.utils.rnn.pad_sequence(value, batch_first=True)
            if k in {"masks", "keypoints", "bboxes", "cls", "segments", "obb"}:
                value = torch.cat(value, 0)
            new_batch[k] = value
        new_batch["batch_idx"] = list(new_batch["batch_idx"])
        for i in range(len(new_batch["batch_idx"])):
            new_batch["batch_idx"][i] += i  # è®©æ¯ä¸ªç›®æ ‡ç¼–å·åŠ ä¸Šæ ·æœ¬åç§»
        new_batch["batch_idx"] = torch.cat(new_batch["batch_idx"], 0)
        return new_batch


class YOLOMultiModalDataset(YOLODataset):
    """
    å¤šæ¨¡æ€æ•°æ®é›†ï¼šåœ¨ YOLODataset åŸºç¡€ä¸Šï¼Œé¢å¤–å¼•å…¥æ–‡æœ¬ä¿¡æ¯(ç±»ååŒä¹‰è¯ç­‰)ï¼Œ
    ä»¥æ”¯æŒå›¾æ–‡è”åˆè®­ç»ƒ(å¦‚ grounding/å¯¹æ¯”å­¦ä¹ ç­‰)ã€‚
    """

    def __init__(self, *args, data: dict | None = None, task: str = "detect", **kwargs):
        super().__init__(*args, data=data, task=task, **kwargs)

    def update_labels_info(self, label: dict) -> dict:
        """
        åœ¨çˆ¶ç±» Instances åŸºç¡€ä¸Šï¼Œç»™æ¯æ¡æ ·æœ¬è¡¥å…… 'texts' å­—æ®µï¼š
        - data["names"] çš„æ¯ä¸ªç±»åˆ«å¯ä»¥ç”¨ "/" è¿æ¥å¤šä¸ªåŒä¹‰è¯ï¼ŒRandomLoadText ä¼šéšæœºé€‰æ‹©å…¶ä¸€ã€‚
        """
        labels = super().update_labels_info(label)
        labels["texts"] = [v.split("/") for _, v in self.data["names"].items()]
        return labels

    def build_transforms(self, hyp: dict | None = None) -> Compose:
        """
        è®­ç»ƒæ—¶æ’å…¥ RandomLoadTextï¼Œç”¨äºåœ¨çº¿é‡‡æ ·æ–‡æœ¬(åŒ…å«è´Ÿæ ·æœ¬å¡«å……)ã€‚
        """
        transforms = super().build_transforms(hyp)
        if self.augment:
            transform = RandomLoadText(
                max_samples=min(self.data["nc"], 80),
                padding=True,
                padding_value=self._get_neg_texts(self.category_freq),
            )
            # æ’å…¥åˆ° Format ä¹‹å‰
            transforms.insert(-1, transform)
        return transforms

    @property
    def category_names(self):
        """è¿”å›ç±»åˆ«åé›†åˆ(æ‹†åˆ†åŒä¹‰è¯ï¼Œå»ç©ºæ ¼)ã€‚"""
        names = self.data["names"].values()
        return {n.strip() for name in names for n in name.split("/")}

    @property
    def category_freq(self):
        """ç»Ÿè®¡æ¯ä¸ªç±»å(åŠåŒä¹‰è¯)åœ¨å½“å‰ labels ä¸­å‡ºç°çš„é¢‘æ¬¡ã€‚"""
        texts = [v.split("/") for v in self.data["names"].values()]
        category_freq = defaultdict(int)
        for label in self.labels:
            for c in label["cls"].squeeze(-1):
                text = texts[int(c)]
                for t in text:
                    t = t.strip()
                    category_freq[t] += 1
        return category_freq

    @staticmethod
    def _get_neg_texts(category_freq: dict, threshold: int = 100) -> list[str]:
        """é€‰æ‹©é«˜é¢‘è¯ä½œä¸ºè´Ÿæ ·æœ¬å¡«å……çš„å€™é€‰(ä¸Šé™ 100)ã€‚"""
        threshold = min(max(category_freq.values()), 100)
        return [k for k, v in category_freq.items() if v >= threshold]


class GroundingDataset(YOLODataset):
    """
    åŸºäº JSON(grounding æ ¼å¼)è¯»å–æ ‡æ³¨çš„æ£€æµ‹/åˆ†å‰²æ•°æ®é›†ã€‚
    - ä¸ YOLO æ–‡æœ¬æ ‡æ³¨ä¸åŒï¼Œè¿™é‡Œä»ä¸€ä¸ª JSON(åŒ…å« images/annotations)ä¸­è§£æå¾—åˆ°ã€‚
    - æ”¯æŒä¸ºæ¯ä¸ª bbox é™„å¸¦æ¥è‡ª caption çš„æ–‡æœ¬ç‰‡æ®µï¼Œç”¨äº grounding/çŸ­è¯­å®šä½è®­ç»ƒã€‚
    """

    def __init__(self, *args, task: str = "detect", json_file: str = "", max_samples: int = 80, **kwargs):
        # ä»…æ”¯æŒ detect/segment ä¸¤ç§ä»»åŠ¡
        assert task in {"detect", "segment"}, "GroundingDataset currently only supports `detect` and `segment` tasks"
        self.json_file = json_file
        self.max_samples = max_samples
        # è¿™é‡Œ data åªè®¾ç½®äº† channels=3ï¼Œç±»åˆ«åç”± JSON è§£ææ—¶åŠ¨æ€ç”Ÿæˆ
        super().__init__(*args, task=task, data={"channels": 3}, **kwargs)

    def get_img_files(self, img_path: str) -> list:
        """è¦†å†™ï¼šå›¾åƒæ–‡ä»¶åˆ—è¡¨åœ¨ get_labels ä¸­ç”± JSON åŠ¨æ€è¯»å–ï¼Œè¿™é‡Œè¿”å›ç©ºåˆ—è¡¨ã€‚"""
        return []

    def verify_labels(self, labels: list[dict[str, Any]]) -> None:
        """
        å¯é€‰çš„æ•°æ®å®Œæ•´æ€§éªŒè¯ï¼šé’ˆå¯¹å·²çŸ¥æ•°æ®åï¼Œç»Ÿè®¡ bbox å®ä¾‹æ•°æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚
        æœªåŒ¹é…çš„æ•°æ®é›†ä¼šè·³è¿‡ã€‚
        """
        expected_counts = {
            "final_mixed_train_no_coco_segm": 3662412,
            "final_mixed_train_no_coco": 3681235,
            "final_flickr_separateGT_train_segm": 638214,
            "final_flickr_separateGT_train": 640704,
        }

        instance_count = sum(label["bboxes"].shape[0] for label in labels)
        for data_name, count in expected_counts.items():
            if data_name in self.json_file:
                assert instance_count == count, f"'{self.json_file}' has {instance_count} instances, expected {count}."
                return
        LOGGER.warning(f"Skipping instance count verification for unrecognized dataset '{self.json_file}'")

    def cache_labels(self, path: Path = Path("./labels.cache")) -> dict[str, Any]:
        """
        ä» JSON è¯»å– annotationsï¼Œè¿‡æ»¤ crowd/æ— æ•ˆæ¡†ï¼Œå½’ä¸€åŒ– bboxï¼Œ
        å¹¶å°† segmentation(è‹¥æœ‰)è½¬æ¢ä¸º boxes æˆ–å¤šè¾¹å½¢ç‚¹åºåˆ—ï¼›åŒæ—¶æŠ½å–æ–‡æœ¬ç‰‡æ®µã€‚
        """
        x = {"labels": []}
        LOGGER.info("Loading annotation file...")
        with open(self.json_file) as f:
            annotations = json.load(f)
        images = {f"{x['id']:d}": x for x in annotations["images"]}
        img_to_anns = defaultdict(list)
        for ann in annotations["annotations"]:
            img_to_anns[ann["image_id"]].append(ann)
        for img_id, anns in TQDM(img_to_anns.items(), desc=f"Reading annotations {self.json_file}"):
            img = images[f"{img_id:d}"]
            h, w, f = img["height"], img["width"], img["file_name"]
            im_file = Path(self.img_path) / f
            if not im_file.exists():
                continue
            self.im_files.append(str(im_file))
            bboxes = []
            segments = []
            cat2id = {}
            texts = []
            for ann in anns:
                if ann["iscrowd"]:
                    continue
                box = np.array(ann["bbox"], dtype=np.float32)
                # COCO: [x,y,w,h] -> ä¸­å¿ƒç‚¹ xy + whï¼Œå¹¶åšå½’ä¸€åŒ–
                box[:2] += box[2:] / 2
                box[[0, 2]] /= float(w)
                box[[1, 3]] /= float(h)
                if box[2] <= 0 or box[3] <= 0:
                    continue

                caption = img["caption"]
                # tokens_positive ç»™å‡º caption çš„å­—ç¬¦åŒºé—´ï¼›æ‹¼æ¥æˆç±»åˆ«åç§°
                cat_name = " ".join([caption[t[0] : t[1]] for t in ann["tokens_positive"]]).lower().strip()
                if not cat_name:
                    continue

                if cat_name not in cat2id:
                    cat2id[cat_name] = len(cat2id)
                    texts.append([cat_name])
                cls = cat2id[cat_name]
                box = [cls] + box.tolist()
                if box not in bboxes:
                    bboxes.append(box)
                    # å¤„ç† segmentationï¼šå¯èƒ½ä¸ºå¤šæ®µï¼Œéœ€è¦åˆå¹¶ä¸å½’ä¸€åŒ–
                    if ann.get("segmentation") is not None:
                        if len(ann["segmentation"]) == 0:
                            segments.append(box)
                            continue
                        elif len(ann["segmentation"]) > 1:
                            s = merge_multi_segment(ann["segmentation"])
                            s = (np.concatenate(s, axis=0) / np.array([w, h], dtype=np.float32)).reshape(-1).tolist()
                        else:
                            s = [j for i in ann["segmentation"] for j in i]
                            s = (
                                (np.array(s, dtype=np.float32).reshape(-1, 2) / np.array([w, h], dtype=np.float32))
                                .reshape(-1)
                                .tolist()
                            )
                        s = [cls] + s
                        segments.append(s)
            lb = np.array(bboxes, dtype=np.float32) if len(bboxes) else np.zeros((0, 5), dtype=np.float32)

            if segments:
                # è‹¥æœ‰å¤šè¾¹å½¢åˆ†å‰²ï¼Œåˆ™ç”± segments åæ¨ xywh æ¡†(ä¿æŒä¸ YOLO æ¥å£ä¸€è‡´)
                classes = np.array([x[0] for x in segments], dtype=np.float32)
                segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in segments]
                lb = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)
            lb = np.array(lb, dtype=np.float32)

            x["labels"].append(
                {
                    "im_file": im_file,
                    "shape": (h, w),
                    "cls": lb[:, 0:1],
                    "bboxes": lb[:, 1:],
                    "segments": segments,
                    "normalized": True,
                    "bbox_format": "xywh",
                    "texts": texts,
                }
            )
        x["hash"] = get_hash(self.json_file)
        save_dataset_cache_file(self.prefix, path, x, DATASET_CACHE_VERSION)
        return x

    def get_labels(self) -> list[dict]:
        """ä¼˜å…ˆä»ç¼“å­˜åŠ è½½ï¼›è‹¥ç¼“å­˜ç¼ºå¤±/ä¸åŒ¹é…åˆ™é‡å»ºã€‚å¹¶åšå®ä¾‹æ•°éªŒè¯ä¸æ—¥å¿—æç¤ºã€‚"""
        cache_path = Path(self.json_file).with_suffix(".cache")
        try:
            cache, _ = load_dataset_cache_file(cache_path), True
            assert cache["version"] == DATASET_CACHE_VERSION
            assert cache["hash"] == get_hash(self.json_file)
        except (FileNotFoundError, AssertionError, AttributeError, ModuleNotFoundError):
            cache, _ = self.cache_labels(cache_path), False
        [cache.pop(k) for k in ("hash", "version")]
        labels = cache["labels"]
        self.verify_labels(labels)
        self.im_files = [str(label["im_file"]) for label in labels]
        if LOCAL_RANK in {-1, 0}:
            LOGGER.info(f"Load {self.json_file} from cache file {cache_path}")
        return labels

    def build_transforms(self, hyp: dict | None = None) -> Compose:
        """ä¸å¤šæ¨¡æ€ç±»ä¼¼ï¼šè®­ç»ƒé˜¶æ®µæ’å…¥ RandomLoadTextï¼Œç”¨äºè´Ÿé‡‡æ ·ä¸æ–‡æœ¬å¢å¼ºã€‚"""
        transforms = super().build_transforms(hyp)
        if self.augment:
            transform = RandomLoadText(
                max_samples=min(self.max_samples, 80),
                padding=True,
                padding_value=self._get_neg_texts(self.category_freq),
            )
            transforms.insert(-1, transform)
        return transforms

    @property
    def category_names(self):
        """ä» labels['texts'] èšåˆæ‰€æœ‰æ–‡æœ¬ç±»å(å»é‡+strip)ã€‚"""
        return {t.strip() for label in self.labels for text in label["texts"] for t in text}

    @property
    def category_freq(self):
        """ç»Ÿè®¡æ–‡æœ¬ç±»åˆ«çš„å‡ºç°é¢‘æ¬¡ï¼Œç”¨äºè´Ÿæ ·æœ¬é˜ˆå€¼ã€‚"""
        category_freq = defaultdict(int)
        for label in self.labels:
            for text in label["texts"]:
                for t in text:
                    t = t.strip()
                    category_freq[t] += 1
        return category_freq

    @staticmethod
    def _get_neg_texts(category_freq: dict, threshold: int = 100) -> list[str]:
        """é€‰æ‹©é«˜é¢‘æ–‡æœ¬ç”¨äºå¡«å……è´Ÿæ ·æœ¬(ä¸Šé™ 100)ã€‚"""
        threshold = min(max(category_freq.values()), 100)
        return [k for k, v in category_freq.items() if v >= threshold]


class YOLOConcatDataset(ConcatDataset):
    """
    å¤šæ•°æ®é›†åˆå¹¶ï¼šå°†å¤šä¸ª YOLODataset ç»„æˆä¸€ä¸ªå¤§çš„ Datasetï¼ŒåŒæ—¶å¤ç”¨å…¶ collate_fnã€‚
    """

    @staticmethod
    def collate_fn(batch: list[dict]) -> dict:
        """æ²¿ç”¨ YOLODataset çš„ collate_fnã€‚"""
        return YOLODataset.collate_fn(batch)

    def close_mosaic(self, hyp: dict) -> None:
        """æ‰¹é‡å…³é—­æ¯ä¸ªå­æ•°æ®é›†çš„ mosaic/mixup ç­‰å¢å¼ºã€‚"""
        for dataset in self.datasets:
            if not hasattr(dataset, "close_mosaic"):
                continue
            dataset.close_mosaic(hyp)


# TODO: æ”¯æŒè¯­ä¹‰åˆ†å‰²ä¸“ç”¨æ•°æ®é›†(ç›®å‰å ä½)
class SemanticDataset(BaseDataset):
    """è¯­ä¹‰åˆ†å‰²æ•°æ®é›†å ä½ç±»ã€‚"""

    def __init__(self):
        super().__init__()


class ClassificationDataset:
    """
    å›¾åƒåˆ†ç±»æ•°æ®é›†ï¼šåŸºäº torchvision çš„ ImageFolderï¼Œ
    é¢å¤–æ”¯æŒï¼š
    - å¯é€‰çš„æ•°æ®ç¼“å­˜(RAM/ç£ç›˜ *.npy)
    - è®­ç»ƒå¢å¼º(AutoAugment/éšæœºæ“¦é™¤/HFlip/VFlip/HSV ç­‰)
    - å›¾åƒåˆæ³•æ€§å¿«é€Ÿæ ¡éªŒä¸ *.cache å­˜å–
    """

    def __init__(self, root: str, args, augment: bool = False, prefix: str = ""):
        import torchvision  # å»¶è¿Ÿå¯¼å…¥ä»¥åŠ é€Ÿ ultralytics çš„æ•´ä½“ import

        # torchvision 0.18+ æ”¯æŒ allow_emptyï¼Œé¿å…ç©ºç±»æŠ¥é”™
        if TORCHVISION_0_18:
            self.base = torchvision.datasets.ImageFolder(root=root, allow_empty=True)
        else:
            self.base = torchvision.datasets.ImageFolder(root=root)
        self.samples = self.base.samples  # [(filepath, class_idx), ...]
        self.root = self.base.root

        # è®­ç»ƒæŠ½æ ·(fraction < 1.0 å¯å‡å°‘æ ·æœ¬é‡ç”¨äºå¿«é€Ÿå®éªŒ)
        if augment and args.fraction < 1.0:
            self.samples = self.samples[: round(len(self.samples) * args.fraction)]
        self.prefix = colorstr(f"{prefix}: ") if prefix else ""

        # ç¼“å­˜ç­–ç•¥ï¼šRAM æˆ–ç£ç›˜(æ³¨æ„ï¼šRAM æ¨¡å¼å­˜åœ¨å†å²å†…å­˜æ³„æ¼ issueï¼Œè¿™é‡Œå¼ºåˆ¶å…³é—­)
        self.cache_ram = args.cache is True or str(args.cache).lower() == "ram"
        if self.cache_ram:
            LOGGER.warning(
                "Classification `cache_ram` training has known memory leak in "
                "https://github.com/ultralytics/ultralytics/issues/9824, setting `cache_ram=False`."
            )
            self.cache_ram = False
        self.cache_disk = str(args.cache).lower() == "disk"

        # å…ˆåšå›¾åƒåˆæ³•æ€§æ ¡éªŒï¼Œè¿”å›è¿‡æ»¤åçš„ samples
        self.samples = self.verify_images()
        # ä¸ºæ¯ä¸ªæ ·æœ¬è®°å½•å¯¹åº”çš„ npy è·¯å¾„ä¸ç¼“å­˜å›¾åƒä½(RAMæ¨¡å¼)
        self.samples = [list(x) + [Path(x[0]).with_suffix(".npy"), None] for x in self.samples]

        # æ„å»ºå¢å¼º/é¢„å¤„ç†æµæ°´çº¿
        scale = (1.0 - args.scale, 1.0)
        self.torch_transforms = (
            classify_augmentations(
                size=args.imgsz,
                scale=scale,
                hflip=args.fliplr,
                vflip=args.flipud,
                erasing=args.erasing,
                auto_augment=args.auto_augment,
                hsv_h=args.hsv_h,
                hsv_s=args.hsv_s,
                hsv_v=args.hsv_v,
            )
            if augment
            else classify_transforms(size=args.imgsz)
        )

    def __getitem__(self, i: int) -> dict:
        """
        è¯»å–ç¬¬ i ä¸ªæ ·æœ¬ï¼š
        - ä¾æ®ç¼“å­˜ç­–ç•¥é€‰æ‹©è¯»å–æº(RAM/ç£ç›˜ *.npy/ç›´æ¥ cv2.imread)
        - è½¬ä¸º PILï¼Œå†èµ° torchvision çš„ transforms
        - è¿”å› {"img": tensor, "cls": class_index}
        """
        f, j, fn, im = self.samples[i]  # æ–‡ä»¶åã€ç±»ç´¢å¼•ã€npy è·¯å¾„ã€RAM ç¼“å­˜å›¾åƒ
        if self.cache_ram:
            if im is None:
                im = self.samples[i][3] = cv2.imread(f)
        elif self.cache_disk:
            if not fn.exists():
                np.save(fn.as_posix(), cv2.imread(f), allow_pickle=False)
            im = np.load(fn)
        else:
            im = cv2.imread(f)  # BGR
        # OpenCV -> PIL(RGB)
        im = Image.fromarray(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))
        sample = self.torch_transforms(im)
        return {"img": sample, "cls": j}

    def __len__(self) -> int:
        return len(self.samples)

    def verify_images(self) -> list[tuple]:
        """
        æ‰«æå¹¶è¿‡æ»¤ä¸å¯è¯»/æŸåå›¾åƒï¼›ä½¿ç”¨ *.cache åŠ é€Ÿä¸‹æ¬¡å¯åŠ¨ã€‚
        - check_file_speedsï¼šæŠ½æ ·æµ‹é€Ÿã€å¿«é€Ÿæ¢æµ‹ I/O é—®é¢˜
        - load/save_dataset_cache_fileï¼šè¯»å–/å†™å…¥ç¼“å­˜(å«å“ˆå¸Œä¸æ•°é‡)
        - è¿”å›è¿‡æ»¤åçš„ [(filepath, class_idx), ...]
        """
        desc = f"{self.prefix}Scanning {self.root}..."
        path = Path(self.root).with_suffix(".cache")  # *.cache è·¯å¾„

        try:
            # å°è¯•è¯»å–ç¼“å­˜å¹¶æ ¡éªŒç‰ˆæœ¬ä¸å“ˆå¸Œ
            check_file_speeds([file for (file, _) in self.samples[:5]], prefix=self.prefix)
            cache = load_dataset_cache_file(path)
            assert cache["version"] == DATASET_CACHE_VERSION
            assert cache["hash"] == get_hash([x[0] for x in self.samples])
            nf, nc, n, samples = cache.pop("results")
            if LOCAL_RANK in {-1, 0}:
                d = f"{desc} {nf} images, {nc} corrupt"
                TQDM(None, desc=d, total=n, initial=n)
                if cache["msgs"]:
                    LOGGER.info("\n".join(cache["msgs"]))
            return samples

        except (FileNotFoundError, AssertionError, AttributeError):
            # ç¼“å­˜ä¸å¯ç”¨ï¼Œåˆ™é€ä¸ªæ ¡éªŒ
            nf, nc, msgs, samples, x = 0, 0, [], [], {}
            with ThreadPool(NUM_THREADS) as pool:
                results = pool.imap(func=verify_image, iterable=zip(self.samples, repeat(self.prefix)))
                pbar = TQDM(results, desc=desc, total=len(self.samples))
                for sample, nf_f, nc_f, msg in pbar:
                    if nf_f:
                        samples.append(sample)
                    if msg:
                        msgs.append(msg)
                    nf += nf_f
                    nc += nc_f
                    pbar.desc = f"{desc} {nf} images, {nc} corrupt"
                pbar.close()
            if msgs:
                LOGGER.info("\n".join(msgs))
            x["hash"] = get_hash([x[0] for x in self.samples])
            x["results"] = nf, nc, len(samples), samples
            x["msgs"] = msgs
            save_dataset_cache_file(self.prefix, path, x, DATASET_CACHE_VERSION)
            return samples
